# ğŸ—ï¸ Indecimal RAG â€“ Grounded AI Assistant for Construction Documents

**Indecimal RAG** is a fully runnable Retrieval-Augmented Generation (RAG) system designed for a construction marketplace.

It enables an AI assistant to answer user queries strictly using internal documents such as policies, FAQs, pricing details, and quality standardsâ€”without relying on the modelâ€™s general knowledge.

The system combines **semantic search** (FAISS + Sentence Transformers) with a **locally hosted LLM** (Qwen 2.5 via Ollama) to ensure responses are accurate, explainable, and hallucination-free.

---

## ğŸš€ Key Features

- ğŸ“„ **Semantic retrieval** from internal construction documents
- ğŸ” **FAISS-based vector search** with cosine similarity
- ğŸ§  **Local LLM inference** using Ollama (no external APIs)
- ğŸ›¡ï¸ **Strict grounding** to retrieved context (zero hallucination design)
- ğŸ§¾ **Transparent display** of retrieved chunks used for each answer
- ğŸ’¬ **Interactive chatbot interface** built with Streamlit
- âš¡ **Optimized** for low-latency local execution

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
| :--- | :--- |
| **Embeddings** | Sentence Transformers (all-MiniLM-L6-v2) |
| **Vector Search** | FAISS (IndexFlatIP â€“ cosine similarity) |
| **LLM** | Qwen 2.5 (1.5B Instruct) via Ollama |
| **Backend Logic** | Python |
| **UI** | Streamlit |
| **Environment** | uv (dependency & lock management) |

---

## âœ… Prerequisites

- Python **3.10+**
- **Ollama** installed and running locally
- CPU-based inference (GPU optional, not required)
- Internet access for first-time model download


## ğŸ§  Models Used

### ğŸ”¹ Embedding Model
**Model:** `all-MiniLM-L6-v2`
* **Why chosen:**
    * Lightweight and fast for local usage
    * Strong semantic similarity performance
    * Ideal for cosine similarity search with FAISS
    * Widely adopted in production RAG pipelines

### ğŸ”¹ Large Language Model (LLM)
**Model:** `qwen2.5:1.5b-instruct`
* **Runtime:** Local inference using Ollama
* **Why chosen:**
    * Instruction-tuned for strict prompt adherence
    * Faster than larger models on CPU
    * Lower hallucination risk
    * Well-suited for extractive, grounded answers

> **Note:** The LLM is used only for answer generation, never for retrieval.

---

## ğŸ“„ Document Chunking & Processing

- Input documents are **Markdown (.md)** files
- Chunking follows **semantic document structure**, not fixed token sizes:
    - Document title â†’ separate chunk
    - Document metadata â†’ separate chunk
    - Each `##` section â†’ individual chunk
- Each chunk includes metadata:
    - Source document
    - Section heading

**This ensures:**
* Meaningful, human-readable chunks
* High-quality retrieval
* Transparent source attribution

---

## ğŸ” Retrieval Pipeline

1. User query is normalized
2. Intent is detected (e.g., payments, delays, accountability)
3. Underspecified queries are expanded using canonical phrasing
4. Query embedding is generated
5. Top-K chunks are retrieved via FAISS
6. Metadata-only chunks are excluded
7. Policy documents are prioritized for enforcement-related queries

*All retrieval happens locally and is deterministic.*

---

## ğŸ›¡ï¸ Grounding & Hallucination Control

Grounding is enforced at three levels:

### 1ï¸âƒ£ Retrieval Constraint
Only retrieved document chunks are passed to the LLM. The model never sees full documents or external knowledge.

### 2ï¸âƒ£ Strict Prompt Rules
The LLM is explicitly instructed to:
* Use only the retrieved context
* Avoid inference or interpretation
* Avoid adding benefits, reasons, or outcomes
* Refuse to answer if information is missing

**Exact refusal response:**
> â€œThe provided documents do not contain this information.â€

### 3ï¸âƒ£ Output Style Enforcement
* Short, factual statements
* Bullet points for multiple mechanisms
* No combined reasoning or assumptions

*This guarantees hallucination-free answers by design.*

---

## ğŸ–¥ï¸ User Interface

Built using **Streamlit**.

**Displays:**
* User query
* Retrieved document chunks
* Final grounded answer

*Demonstrates full end-to-end RAG behavior (not a PoC).*

---

## ğŸ“ Project Structure

```text
RAG/
â”œâ”€â”€ rag_pipeline/
â”‚   â”œâ”€â”€ ingestion_chunking.py
â”‚   â”œâ”€â”€ embedding_indexing.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ generation.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # Input markdown documents
â”‚   â””â”€â”€ processed/    # Chunks, embeddings, FAISS index
â”‚
â”œâ”€â”€ app.py            # Streamlit app
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
## âš™ï¸ Ollama Installation (Required)

### ğŸ§ Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh

```

### ğŸ macOS
Download and install from:
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)
*(Ollama runs in the background after installation)*

### ğŸªŸ Windows
Download and install from:
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

### ğŸ“¥ Pull Required Model
```bash
ollama pull qwen2.5:1.5b-instruct
```

## â–¶ï¸ How to Run the Project

### 1. Environment Setup
```bash
uv venv
source .venv/bin/activate
uv pip sync
```

### 2. Run Ingestion & Indexing
```bash
python rag_pipeline/ingestion_chunking.py
python rag_pipeline/embedding_indexing.py
``` 

### 3. Start the App
```bash
streamlit run app.py
```

## ğŸ§ª Evaluation & Testing

The system was tested with:
* Fact-based queries present in documents
* Queries missing from documents (refusal behavior)
* Accountability and delay-related questions
* Pricing and payment-related questions

**Observed behavior:**
* âœ… Accurate retrieval
* âœ… Correct grounded answers
* âœ… Explicit refusal when data is missing
* âœ… No hallucinated responses

---

## ğŸ‘¤ Author

**Devansh Kumar Sinha**
B.Tech Computer Science
Focused on AI Systems, RAG, and LLM Engineering

---

## ğŸ“œ License

This project is licensed under the **MIT License**.